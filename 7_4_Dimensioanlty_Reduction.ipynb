{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41dc81ec-c39a-49f7-8397-39f252760e37",
      "metadata": {
        "id": "41dc81ec-c39a-49f7-8397-39f252760e37"
      },
      "source": [
        "# Dimensionality Reduction with PCA\n",
        "\n",
        "Dimensionality reduction is a fundamental technique in machine learning and data science that reduces the number of features (or variables) in a dataset while retaining as much meaningful information as possible.\n",
        "\n",
        "High-dimensional data can be computationally expensive, prone to overfitting, and difficult to interpret. Dimensionality reduction helps address these challenges by simplifying the data without significantly sacrificing its structure or patterns.\n",
        "\n",
        "In this notebook, we focus on **Principal Component Analysis (PCA)**, one of the most widely used techniques for dimensionality reduction. PCA transforms the data into a set of new features, called principal components, that capture the largest variance in the data.\n",
        "\n",
        "We will use the Breast Cancer dataset to:\n",
        "1. Train a machine learning model (Random Forest classifier) on the original dataset with 30 features.\n",
        "2. Apply PCA to reduce the dataset dimensions.\n",
        "3. Train the same model on the reduced dataset.\n",
        "4. Compare the performance of the model before and after applying PCA.\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Understand how PCA works in simplifying datasets.\n",
        "- Learn how dimensionality reduction impacts model performance.\n",
        "- See a practical implementation of PCA with Python.\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Libraries and Load Dataset\n",
        "\n",
        "In this step, we import the necessary libraries and load the Breast Cancer dataset. This dataset contains 30 features describing tumor characteristics (e.g., radius, texture). We will split the data into `X` (features) and `Y` (target variable) to prepare for modeling.\n"
      ],
      "metadata": {
        "id": "s0H5a5e3jQLD"
      },
      "id": "s0H5a5e3jQLD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e341c39b-c021-46fd-a79e-56bcde01506a",
      "metadata": {
        "id": "e341c39b-c021-46fd-a79e-56bcde01506a"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# 1: Import libraries, load the dataset, and create X and Y\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "lbc = load_breast_cancer()\n",
        "\n",
        "# Features (X) and Target (Y)\n",
        "X = pd.DataFrame(lbc['data'], columns=lbc['feature_names'])\n",
        "Y = pd.DataFrame(lbc['target'], columns=['type'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7c4a17-9a7b-4189-87ad-79a3efb2a8a9",
      "metadata": {
        "id": "1c7c4a17-9a7b-4189-87ad-79a3efb2a8a9"
      },
      "source": [
        "### Step 2: Train Model Without PCA\n",
        "\n",
        "Here, we train a Random Forest classifier using the original dataset with all 30 features. This step will help us evaluate the model's performance before applying PCA. We split the data into training and testing sets, train the classifier, and evaluate it using metrics like confusion matrix and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7e7479-8b06-49a3-86c5-2da21f69bed1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa7e7479-8b06-49a3-86c5-2da21f69bed1",
        "outputId": "eba519cb-0597-4112-ab6a-befb14235373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (Without PCA):\n",
            "[[ 61   3]\n",
            " [  0 107]]\n",
            "Accuracy (Without PCA): 0.9825\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# 2: Perform Prediction Without PCA\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1234, stratify=Y)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc1 = RandomForestClassifier(random_state=1234)\n",
        "rfc1.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions\n",
        "Y_predict1 = rfc1.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm1 = confusion_matrix(Y_test, Y_predict1)\n",
        "score1 = rfc1.score(X_test, Y_test)\n",
        "\n",
        "print(\"Confusion Matrix (Without PCA):\")\n",
        "print(cm1)\n",
        "print(f\"Accuracy (Without PCA): {score1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49ad2e2-a3c8-4efd-81a8-df90b759b962",
      "metadata": {
        "id": "c49ad2e2-a3c8-4efd-81a8-df90b759b962"
      },
      "source": [
        "### Step 3: Normalize Data\n",
        "\n",
        "Before applying PCA, we normalize the data to ensure all features have zero mean and unit variance. Normalization is necessary because PCA relies on variance, and features with larger magnitudes can dominate the results if the data isn't standardized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5d9274-6aad-4661-b93c-1b8d48558b57",
      "metadata": {
        "id": "1c5d9274-6aad-4661-b93c-1b8d48558b57"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# 3: Perform PCA for Dimensionality Reduction\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar = StandardScaler()\n",
        "X_scaled = scalar.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ef619b-cd6a-4c16-8bc2-75886e915c24",
      "metadata": {
        "id": "38ef619b-cd6a-4c16-8bc2-75886e915c24"
      },
      "source": [
        "### Step 4: Apply PCA\n",
        "\n",
        "Using PCA, we reduce the dataset from 30 features to 5 principal components. These components capture the majority of the variance in the dataset, allowing us to retain the most important patterns while discarding redundant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ada3a47-b701-41ef-b302-ae37bc36cd0d",
      "metadata": {
        "id": "6ada3a47-b701-41ef-b302-ae37bc36cd0d"
      },
      "outputs": [],
      "source": [
        "# Apply PCA and reduce dimensions to 5 principal components\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=5)\n",
        "X_pca = pca.fit_transform(X_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f42127-6903-4789-bcc8-a11c691e1ac1",
      "metadata": {
        "id": "52f42127-6903-4789-bcc8-a11c691e1ac1"
      },
      "source": [
        "### Step 5: Train Model After PCA\n",
        "\n",
        "After reducing the dimensions using PCA, we retrain the Random Forest classifier on the transformed dataset (with 5 principal components). This step allows us to compare the model's performance before and after applying PCA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c92f20-6cc2-41fb-b6dc-8d0e8a1eb13d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35c92f20-6cc2-41fb-b6dc-8d0e8a1eb13d",
        "outputId": "b897fc32-8be1-4cbf-f6b6-64dbf9f3354d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (With PCA):\n",
            "[[ 61   3]\n",
            " [  1 106]]\n",
            "Accuracy (With PCA): 0.9766\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Step 4: Perform Prediction After PCA\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Split the reduced dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=1234, stratify=Y)\n",
        "\n",
        "# Train a Random Forest Classifier on PCA-transformed data\n",
        "rfc2 = RandomForestClassifier(random_state=1234)\n",
        "rfc2.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions\n",
        "Y_predict2 = rfc2.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "cm2 = confusion_matrix(Y_test, Y_predict2)\n",
        "score2 = rfc2.score(X_test, Y_test)\n",
        "\n",
        "print(\"Confusion Matrix (With PCA):\")\n",
        "print(cm2)\n",
        "print(f\"Accuracy (With PCA): {score2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df7e882-5b3c-4f82-86e0-0f40535e5d85",
      "metadata": {
        "id": "7df7e882-5b3c-4f82-86e0-0f40535e5d85"
      },
      "source": [
        "### Step 6: Compare Results\n",
        "\n",
        "In this final step, we compare the model's performance before and after PCA. We observe changes in accuracy and discuss how dimensionality reduction impacts computational efficiency and trade-offs in model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d78b95-8f8f-4be8-b48e-bede1883bcf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d78b95-8f8f-4be8-b48e-bede1883bcf6",
        "outputId": "7b016616-a49f-47e3-ee72-3c74b3a7f10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Results:\n",
            "Accuracy Without PCA: 0.9825\n",
            "Accuracy With PCA: 0.9766\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Step 5: Compare Results\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "print(\"Comparison of Results:\")\n",
        "print(f\"Accuracy Without PCA: {score1:.4f}\")\n",
        "print(f\"Accuracy With PCA: {score2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e36cf7e-17bd-446b-a3d1-7b5b8b5b50ec",
      "metadata": {
        "id": "8e36cf7e-17bd-446b-a3d1-7b5b8b5b50ec"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "This notebook demonstrates how PCA simplifies datasets for machine learning tasks. Although dimensionality reduction can lead to a slight decrease in accuracy, it significantly reduces overfitting, computational costs, and complexity when working with high-dimensional datasets. PCA is an essential technique for handling real-world data with numerous features.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}